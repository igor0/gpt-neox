{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfd86d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ruamel.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c68599",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "\n",
    "import torch\n",
    "import glob\n",
    "import os\n",
    "#torch.load(\"\")\n",
    "\n",
    "def go(layers):\n",
    "    for f in glob.glob(\"/mnt/ssd-1/igor/gpt-neox/models/dense_large.logit_lens.{}/*/*.pt\".format(layers)):\n",
    "        ch = torch.load(f, map_location=\"cpu\")\n",
    "        if 'args' in ch:\n",
    "            del ch['optimizer']\n",
    "            ch['args']['num_layers'] = layers\n",
    "            os.rename(f, f + \".args_orig\")\n",
    "            torch.save(ch, f)\n",
    "        del ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f97fc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(25):\n",
    "    go(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6958e569",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layers in range(25):\n",
    "    for f in glob.glob(\"/mnt/ssd-1/igor/gpt-neox/models/dense_medium.{}/*/*.pt\".format(layers)):\n",
    "        ch = torch.load(f)\n",
    "ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a28201f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ruamel.yaml\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789f6a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from ruamel.yaml import YAML\n",
    "\n",
    "inp = \"\"\"\\\n",
    "{\n",
    "   # parallelism settings ( you will want to change these based on your cluster setup, ideally scheduling pipeline stages\n",
    "   # across the node boundaries )\n",
    "   \"pipe-parallel-size\": 1,\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "yaml = YAML()\n",
    "code = yaml.load(inp)\n",
    "yaml.dump(code, sys.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef7a879",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "file_name = '/mnt/ssd-1/igor/gpt-neox/models/dense_medium/configs/config.yml'\n",
    "with open(file_name) as conf_file:\n",
    "    conf = yaml.load(conf_file, Loader=yaml.FullLoader)\n",
    "\n",
    "print(conf['num-layers'])\n",
    "conf['num-layers'] = 17\n",
    "conf['load'] = \"mypath\"\n",
    "conf['save'] = \"mypath\"\n",
    "\n",
    "with open('/tmp/test.yml', 'w') as f:\n",
    "    yaml.dump(conf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e820a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename wandb runs\n",
    "\n",
    "import wandb\n",
    "import json\n",
    "import os\n",
    "\n",
    "api = wandb.Api()\n",
    "for run in api.runs(\"igoro/neox\"):\n",
    "    if run.name != \"distill-0-0\":\n",
    "        print(\"Skipping\", run.name)\n",
    "        continue\n",
    "\n",
    "    j = json.loads(run.json_config)\n",
    "    if \"load\" in j and \"value\" in j[\"load\"]:\n",
    "        new_name = os.path.basename(j[\"load\"][\"value\"])\n",
    "    else:\n",
    "        print(\"WARNING: Skipping\", run)\n",
    "        continue\n",
    "    \n",
    "    print(\"Renaming\", run.name)\n",
    "    run.name = new_name\n",
    "    run.update()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfab1731",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import glob\n",
    "import os\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "def show(x, prefix = \"\", path = \"\"):\n",
    "    if type(x) is dict or type(x) is OrderedDict:\n",
    "        print(prefix, path)\n",
    "        for key in x.keys():\n",
    "            show(x[key], prefix + \"  \", str(key))\n",
    "    elif type(x) is torch.Tensor:\n",
    "        print(prefix, path, x.shape, x.dtype)\n",
    "    elif type(x) is list:\n",
    "        print(prefix, path)\n",
    "        for i in range(len(x)):\n",
    "            show(x[i], prefix + \"  \", \"[{}]\".format(i))\n",
    "    else:\n",
    "        print(prefix, path, type(x), x)\n",
    "\n",
    "def go(model, checkpoint):\n",
    "    for f in glob.glob(\"/mnt/ssd-1/igor/gpt-neox/models/{}/{}/*.pt\".format(model, checkpoint)):\n",
    "        ch = torch.load(f, map_location=\"cpu\")\n",
    "        print(\"----\", f)\n",
    "        if 'optimizer' in ch:\n",
    "            #print(f, ch['optimizer'][\"optimizer_state_dict\"].keys())\n",
    "            show(ch)\n",
    "\n",
    "\n",
    "def load(model, checkpoint):\n",
    "    for f in glob.glob(\"/mnt/ssd-1/igor/gpt-neox/models/{}/{}/mp_*.pt\".format(model, checkpoint)):\n",
    "        ch = torch.load(f, map_location=\"cpu\")\n",
    "        return ch\n",
    "#go(\"dense_small.final_linear.7\", \"global_step10000\")\n",
    "#go(\"dense_small.logit_lens.7\", \"global_step485000\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2012370b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(c1)\n",
    "#print(c2)\n",
    "#c2[\"optimizer\"][\"scale_window\"] = c1[\"optimizer\"][\"scale_window\"]\n",
    "\n",
    "\n",
    "c1 = load(\"dense_small.final_linear.7\", \"global_step10000\")\n",
    "c2 = load(\"dense_small.logit_lens.7\", \"global_step485000\")\n",
    "#c2[\"optimizer\"][\"fp32_groups_flat\"] = []\n",
    "torch.save(c2, \"/mnt/ssd-1/igor/gpt-neox/models/dense_small.final_linear.7_exp/baseline/mp_rank_00_model_states.pt\")\n",
    "\n",
    "#shutil.copy(\"/mnt/ssd-1/igor/gpt-neox/models/dense_small.final_linear.7/baseline/mp_rank_00_model_states.pt\", \"/mnt/ssd-1/igor/gpt-neox/models/dense_small.final_linear.7_exp/baseline/\")\n",
    "#shutil.copy(\"/mnt/ssd-1/igor/gpt-neox/models/dense_small.logit_lens.7/global_step485000/mp_rank_00_model_states.pt\", \"/mnt/ssd-1/igor/gpt-neox/models/dense_small.final_linear.7_exp/baseline/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f6a361",
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = load(\"dense_small.final_linear.7\", \"global_step10000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e693f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(c1.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2786ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = torch.load(\"/mnt/ssd-1/igor/gpt-neox/models/dense_small/global_step485000/mp_rank_00_model_states.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e741fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "show(cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40b24d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"dense_small.out_linear_all.0\"\n",
    "checkpoint = \"baseline\"\n",
    "print(\"/mnt/ssd-1/igor/gpt-neox/models/{}/{}/*.pt\".format(model, checkpoint))\n",
    "for f in sorted(glob.glob(\"/mnt/ssd-1/igor/gpt-neox/models/{}/{}/*.pt\".format(model, checkpoint))):\n",
    "    ch = torch.load(f, map_location=\"cpu\")\n",
    "    print(\"----\", f)\n",
    "    show(ch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
